{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "## Prerequisites\n",
    "- [Classification](https://github.com/AI-Core/Strong-ML-Foundations/blob/master/Classification.ipynb)\n",
    "\n",
    "## Why do simple models struggle with meaningful tasks?\n",
    "\n",
    "Whereas the size of a house and its price might be linearly correlated, the pixel intensities of an image are certainly not linearly correlated with whether it contains a dog or a cat.\n",
    "\n",
    "![](./images/complex-fn.png)\n",
    "\n",
    "We need to build much more complex models to solve harder problems.\n",
    "\n",
    "## Can we build more complex models by combining many simple transformations?\n",
    "\n",
    "The models we have just seen apply a single transformation to the data. However most problems of practical interest can't be solved using such simple models. \n",
    "\n",
    "Models with greater **capacity** are those which are able to model more complicated functions.\n",
    "\n",
    "A single linear transformation (multiplication by weights of model) stretches the input space by a certain factor in some direction, and adding a constant (bias) shifts it. \n",
    "We call models which apply  \n",
    "What if we applied more than one **layer** of transformations to our inputs, to create a **deep model**. Would we be able to increase the capacity of our model and make it able to model more complex input-output relationships, particularly non-linear ones?\n",
    "\n",
    "![](./images/shallow-vs-deep.png)\n",
    "\n",
    "...well, not quite yet.\n",
    "\n",
    "...if we repeatedly apply a linear transformation, the input can be factorised out of the output, showing that many repeated linear functions are eventually equal to a single linear transformation.\n",
    "\n",
    "![](./images/factor-proof.png)\n",
    "\n",
    "## So how can we increase the capacity of our models?\n",
    "\n",
    "We want to be able to model non-linear functions, so let's try to throw in some non-linear transformations into our model.\n",
    "\n",
    "![](./images/activation.png)\n",
    "\n",
    "These non-linear functions prevent the input being able to be factorised out of the model. Hence the overall transformation can represent non-linear input-output relationships.\n",
    "\n",
    "We call these non-linear functions **activation functions**.\n",
    "\n",
    "However, It's not like we want to introduce really complicated functions into our model - ideally we wouldn't even have to and we could keep things simple. So let's try and complicate things only a minimal amount by keeping our activation functions very simple.\n",
    "\n",
    "Here are some common activation functions. ReLU (Rectified Linear Unit) is by far the most widely used.\n",
    "\n",
    "![](./images/activ-fns.png)\n",
    "\n",
    "Now we have all of the ingredients to fully understand how we can model more complicated functions. Let's look at that all together:\n",
    "\n",
    "![](./images/full-nn.png)\n",
    "\n",
    "Guess what? That is a **neural network**. Surprise.\n",
    "\n",
    "It's just repeated simple linear transformations followed by simple non-linear transformations (activation functions). Simple.\n",
    "\n",
    "Let's learn some jargon.\n",
    "\n",
    "![](./images/nn.png)\n",
    "\n",
    "Neural networks have additional hyperparameters of the depth of the model and the width of each layer. These \n",
    "\n",
    "## What can neural networks do?\n",
    "\n",
    "The motivation that led us to deriving neural networks was that we wanted to model more complex functions. But what functions can a neural network actually represent? Well, as we show below they can actually represent almost all continuous functions. Neural Networks are **general function approximators**.\n",
    "\n",
    "![](./images/univ-approx.png)\n",
    "\n",
    "## How can our neural networks learn to model some function?\n",
    "\n",
    "As we did in the optimisation notebook, we can adjust our model parameters using gradient descent as such:\n",
    "1. Pass input data forward through model to output a prediction\n",
    "2. Calculate loss between predicted output and output label\n",
    "3. Find direction that moving the model parameters in will reduce the error\n",
    "4. Move model weights (parameters) a small amount in that direction \n",
    "\n",
    "![](./images/backprop.png)\n",
    "\n",
    "Here you can see that many terms reappear when computing the gradients of preceeding layers. \n",
    "By caching those terms, we save having to recompute them for these layers nearer the input. This makes finding the gradients of the loss with respect to each weight in the model much more efficient both in terms of memory and speed. \n",
    "This process of computing these gradients effectively is called the **backpropagation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's prepare our data\n",
    "\n",
    "Today we are going to look at a dataset called MNIST (em-nist). It consists of 70,000 images of hand drawn digits from 0-9. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# GET THE TRAINING DATASET\n",
    "train_data = datasets.MNIST(root='MNIST-data',                        # where is the data (going to be) stored\n",
    "                            transform=transforms.ToTensor(),          # transform the data from a PIL image to a tensor\n",
    "                            train=True,                               # is this training data?\n",
    "                            download=True                             # should i download it if it's not already here?\n",
    "                           )\n",
    "\n",
    "# GET THE TEST DATASET\n",
    "test_data = datasets.MNIST(root='MNIST-data',\n",
    "                           transform=transforms.ToTensor(),\n",
    "                           train=False,\n",
    "                          )\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# PRINT THEIR LENGTHS AND VISUALISE AN EXAMPLE\n",
    "x = train_data[np.random.randint(0, 300)][0]    # get a random example image\n",
    "plt.imshow(x[0].numpy(),cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make training & validation sets using PyTorch's `random_split()`\n",
    "PyTorch has a utility method `torch.utils.data.random_split()` that makes it easy to randomly split a dataset. Check out the [docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.random_split) here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FURTHER SPLIT THE TRAINING INTO TRAINING AND VALIDATION\n",
    "train_data, val_data = torch.utils.data.random_split(train_data, [50000, 10000])    # split into 50K training & 10K validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch's `DataLoader` \n",
    "PyTorch has a handy utility called a `DataLoader` which can pass us our data in mini-batches of a specified batch size. It can also shuffle them for us.\n",
    "\n",
    "Let's use `torch.data.DataLoader` to create data loaders from our train, validation and test datasets now. Hint: look at the [docs](https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "\n",
    "# MAKE TRAINING DATALOADER\n",
    "train_loader = torch.utils.data.DataLoader( # create a data loader\n",
    "    train_data, # what dataset should it sample from?\n",
    "    shuffle=True, # should it shuffle the examples?\n",
    "    batch_size=batch_size # how large should the batches that it samples be?\n",
    ")\n",
    "\n",
    "# MAKE VALIDATION DATALOADER\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")\n",
    "\n",
    "# MAKE TEST DATALOADER\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_data,\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a neural network with PyTorch\n",
    "\n",
    "PyTorch makes it really easy for us to build complex models that can be improved via gradient based optimisation. It does this by providing a class named `torch.nn.Module`. Our model classes should inherit from this class because it does a few very useful things for us:\n",
    "\n",
    "1. `torch.nn.Module` keeps track of all `torch.nn.Parameters` that are created within it. So when we add a linear layer to our model, the parameters (matrix of weights) in that layer will be added to a list of our model's parameters. We can retrieve all parameters of our model using its `parameters()` method. We will later pass this (`mymodel.parameters()`) to our optimiser when we tell it that *this* is what it should be optimising.\n",
    "\n",
    "\n",
    "2. `torch.nn.Module` treats the `forward` method (function) of any child class specially by assigning it to the `__call__` method. That means that running `mymodel.forward(some_data)` is equal to `mymodel(some_data)`. \n",
    "\n",
    "\n",
    "It contains many more useful tools\n",
    "\n",
    "[More detail](https://pytorch.org/tutorials/beginner/nn_tutorial.html) on `torch.nn.Module`\n",
    "Check out the docs [here]()\n",
    "\n",
    "Once we have created a class to represent our model, we need to define how it performs the forward pass. What layers of transformations do we need to give it? \n",
    "Check out these [docs](https://pytorch.org/docs/stable/nn.html#linear-layers) to look at all the layers PyTorch provides.\n",
    "Hint: what layer have I linked to?\n",
    "\n",
    "After we've defined some layers for our model we should implement the forward function that will define what happens when we call an instance of our class. This should pass the argument (our input data) through each of the layers, and apply an activation function to them between each, before returning the transformed input as the output. The output should represent a categorical probability distribution over which class the input belongs to. What shape does it need to be? What function does it need to have applied to it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F \n",
    "class NN(torch.nn.Module): # create a neural network class\n",
    "    def __init__(self): # initialiser\n",
    "        super().__init__() # initialise the parent class\n",
    "        self.layer1 = torch.nn.Linear(784, 1024) # create our first linear layer\n",
    "        self.layer2 = torch.nn.Linear(1024, 256) # create our second linear layer\n",
    "        self.layer3 = torch.nn.Linear(256, 10) # create our third linear layer\n",
    "        \n",
    "    def forward(self, x): # define the forward pass\n",
    "        x = x.view(-1, 784) # flatten out our image features into vectors\n",
    "        x = self.layer1(x) # pass through the first linear layer\n",
    "        x = F.relu(x) # apply activation function\n",
    "        x = self.layer2(x) # pass through the second linear layer\n",
    "        x = F.relu(x) # apply activation function\n",
    "        x = self.layer3(x) # pass through the third linear layer\n",
    "        x = F.softmax(x) # apply activation function\n",
    "        return x # return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the neural network and visualising it's performance\n",
    "\n",
    "Now we've actually made a template for our model, we can actually\n",
    "- instantiate it by creating an instance of it from our class template\n",
    "- define how we will improve it by specifying an optimiser\n",
    "- define how we will measure its performance by specifying a criterion\n",
    "- train it\n",
    "- write its loss to a graph and see how this changes as it continues to train\n",
    "\n",
    "Let's code that up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 0 \tLoss: tensor(2.3029, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 1 \tLoss: tensor(2.2933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 2 \tLoss: tensor(2.2810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 3 \tLoss: tensor(2.2587, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 4 \tLoss: tensor(2.2149, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 5 \tLoss: tensor(2.1891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 6 \tLoss: tensor(2.1407, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 7 \tLoss: tensor(2.0949, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 8 \tLoss: tensor(2.0353, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 9 \tLoss: tensor(1.9884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 10 \tLoss: tensor(1.9110, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 11 \tLoss: tensor(1.8766, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 12 \tLoss: tensor(1.8757, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 13 \tLoss: tensor(1.8035, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 14 \tLoss: tensor(1.8279, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 15 \tLoss: tensor(1.8490, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 16 \tLoss: tensor(1.8065, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 17 \tLoss: tensor(1.7912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 18 \tLoss: tensor(1.7809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 19 \tLoss: tensor(1.8255, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 20 \tLoss: tensor(1.7483, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 21 \tLoss: tensor(1.7301, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 22 \tLoss: tensor(1.7575, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 23 \tLoss: tensor(1.7355, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 24 \tLoss: tensor(1.8315, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 25 \tLoss: tensor(1.7823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 26 \tLoss: tensor(1.7485, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 27 \tLoss: tensor(1.7215, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 28 \tLoss: tensor(1.7708, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 29 \tLoss: tensor(1.7337, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 30 \tLoss: tensor(1.7627, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 31 \tLoss: tensor(1.7444, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 32 \tLoss: tensor(1.7496, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 33 \tLoss: tensor(1.6993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 34 \tLoss: tensor(1.6887, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 35 \tLoss: tensor(1.7117, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 36 \tLoss: tensor(1.6872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 37 \tLoss: tensor(1.6919, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 38 \tLoss: tensor(1.6852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 39 \tLoss: tensor(1.6217, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 40 \tLoss: tensor(1.6952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 41 \tLoss: tensor(1.6752, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 42 \tLoss: tensor(1.6410, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 43 \tLoss: tensor(1.6612, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 44 \tLoss: tensor(1.6391, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 45 \tLoss: tensor(1.6809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 46 \tLoss: tensor(1.6450, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 47 \tLoss: tensor(1.6616, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 48 \tLoss: tensor(1.6636, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 49 \tLoss: tensor(1.6161, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 50 \tLoss: tensor(1.6384, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 51 \tLoss: tensor(1.6294, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 52 \tLoss: tensor(1.6852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 53 \tLoss: tensor(1.6380, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 54 \tLoss: tensor(1.6612, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 55 \tLoss: tensor(1.6732, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 56 \tLoss: tensor(1.6608, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 57 \tLoss: tensor(1.6599, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 58 \tLoss: tensor(1.6677, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 59 \tLoss: tensor(1.6329, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 60 \tLoss: tensor(1.6464, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 61 \tLoss: tensor(1.6142, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 62 \tLoss: tensor(1.6086, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 63 \tLoss: tensor(1.6441, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 64 \tLoss: tensor(1.6153, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 65 \tLoss: tensor(1.6458, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 66 \tLoss: tensor(1.6173, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 67 \tLoss: tensor(1.6494, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 68 \tLoss: tensor(1.6204, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 69 \tLoss: tensor(1.6080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 70 \tLoss: tensor(1.6093, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 71 \tLoss: tensor(1.6357, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 72 \tLoss: tensor(1.6696, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 73 \tLoss: tensor(1.6386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 74 \tLoss: tensor(1.6533, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 75 \tLoss: tensor(1.5902, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 76 \tLoss: tensor(1.6468, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 77 \tLoss: tensor(1.5953, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 78 \tLoss: tensor(1.6188, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 79 \tLoss: tensor(1.6498, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 80 \tLoss: tensor(1.6149, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 81 \tLoss: tensor(1.6242, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 82 \tLoss: tensor(1.5771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 83 \tLoss: tensor(1.6396, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 84 \tLoss: tensor(1.6515, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 85 \tLoss: tensor(1.6188, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 86 \tLoss: tensor(1.6377, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 87 \tLoss: tensor(1.6098, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 88 \tLoss: tensor(1.6107, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 89 \tLoss: tensor(1.6428, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 90 \tLoss: tensor(1.6292, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 91 \tLoss: tensor(1.5933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 92 \tLoss: tensor(1.5928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 93 \tLoss: tensor(1.6481, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 94 \tLoss: tensor(1.5932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 95 \tLoss: tensor(1.6070, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 96 \tLoss: tensor(1.6328, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 97 \tLoss: tensor(1.6107, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 98 \tLoss: tensor(1.5905, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 99 \tLoss: tensor(1.6052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 100 \tLoss: tensor(1.5864, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 101 \tLoss: tensor(1.6051, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 102 \tLoss: tensor(1.5948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 103 \tLoss: tensor(1.5677, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 104 \tLoss: tensor(1.5600, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 105 \tLoss: tensor(1.6127, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 106 \tLoss: tensor(1.5849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 107 \tLoss: tensor(1.5589, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 108 \tLoss: tensor(1.5621, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 109 \tLoss: tensor(1.6049, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 110 \tLoss: tensor(1.5726, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 111 \tLoss: tensor(1.5669, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 112 \tLoss: tensor(1.5771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 113 \tLoss: tensor(1.5744, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 114 \tLoss: tensor(1.5502, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 115 \tLoss: tensor(1.5469, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 116 \tLoss: tensor(1.5736, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 117 \tLoss: tensor(1.6023, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 118 \tLoss: tensor(1.5956, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 119 \tLoss: tensor(1.5523, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 \tBatch: 120 \tLoss: tensor(1.5626, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 121 \tLoss: tensor(1.5657, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 122 \tLoss: tensor(1.5834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 123 \tLoss: tensor(1.5515, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 124 \tLoss: tensor(1.5298, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 125 \tLoss: tensor(1.5714, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 126 \tLoss: tensor(1.5540, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 127 \tLoss: tensor(1.5824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 128 \tLoss: tensor(1.5691, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 129 \tLoss: tensor(1.5525, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 130 \tLoss: tensor(1.5483, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 131 \tLoss: tensor(1.5722, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 132 \tLoss: tensor(1.5733, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 133 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 134 \tLoss: tensor(1.5621, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 135 \tLoss: tensor(1.5579, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 136 \tLoss: tensor(1.5801, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 137 \tLoss: tensor(1.5841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 138 \tLoss: tensor(1.5447, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 139 \tLoss: tensor(1.5933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 140 \tLoss: tensor(1.5674, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 141 \tLoss: tensor(1.4999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 142 \tLoss: tensor(1.5415, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 143 \tLoss: tensor(1.5629, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 144 \tLoss: tensor(1.5361, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 145 \tLoss: tensor(1.5573, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 146 \tLoss: tensor(1.5359, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 147 \tLoss: tensor(1.5475, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 148 \tLoss: tensor(1.5432, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 149 \tLoss: tensor(1.5435, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 150 \tLoss: tensor(1.5585, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 151 \tLoss: tensor(1.5540, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 152 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 153 \tLoss: tensor(1.5637, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 154 \tLoss: tensor(1.5414, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 155 \tLoss: tensor(1.5588, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 156 \tLoss: tensor(1.5550, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 157 \tLoss: tensor(1.5539, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 158 \tLoss: tensor(1.5557, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 159 \tLoss: tensor(1.5434, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 160 \tLoss: tensor(1.5709, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 161 \tLoss: tensor(1.5408, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 162 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 163 \tLoss: tensor(1.5686, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 164 \tLoss: tensor(1.5844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 165 \tLoss: tensor(1.5366, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 166 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 167 \tLoss: tensor(1.5333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 168 \tLoss: tensor(1.5333, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 169 \tLoss: tensor(1.5727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 170 \tLoss: tensor(1.5504, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 171 \tLoss: tensor(1.5538, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 172 \tLoss: tensor(1.5578, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 173 \tLoss: tensor(1.5714, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 174 \tLoss: tensor(1.5464, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 175 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 176 \tLoss: tensor(1.5524, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 177 \tLoss: tensor(1.5421, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 178 \tLoss: tensor(1.5478, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 179 \tLoss: tensor(1.5399, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 180 \tLoss: tensor(1.5213, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 181 \tLoss: tensor(1.5563, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 182 \tLoss: tensor(1.5568, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 183 \tLoss: tensor(1.5354, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 184 \tLoss: tensor(1.5496, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 185 \tLoss: tensor(1.5249, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 186 \tLoss: tensor(1.5291, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 187 \tLoss: tensor(1.5274, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 188 \tLoss: tensor(1.5291, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 189 \tLoss: tensor(1.4947, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 190 \tLoss: tensor(1.5553, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 191 \tLoss: tensor(1.5601, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 192 \tLoss: tensor(1.5502, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 193 \tLoss: tensor(1.5225, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 194 \tLoss: tensor(1.5406, grad_fn=<NllLossBackward>)\n",
      "Epoch: 0 \tBatch: 195 \tLoss: tensor(1.5215, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 0 \tLoss: tensor(1.5542, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 1 \tLoss: tensor(1.5212, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 2 \tLoss: tensor(1.5321, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 3 \tLoss: tensor(1.5214, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 4 \tLoss: tensor(1.5342, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 5 \tLoss: tensor(1.5285, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 6 \tLoss: tensor(1.5351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 7 \tLoss: tensor(1.5176, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 8 \tLoss: tensor(1.5234, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 9 \tLoss: tensor(1.5075, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 10 \tLoss: tensor(1.5466, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 11 \tLoss: tensor(1.5339, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 12 \tLoss: tensor(1.5362, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 13 \tLoss: tensor(1.5379, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 14 \tLoss: tensor(1.5453, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 15 \tLoss: tensor(1.5222, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 16 \tLoss: tensor(1.5274, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 17 \tLoss: tensor(1.5265, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 18 \tLoss: tensor(1.5258, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 19 \tLoss: tensor(1.5175, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 20 \tLoss: tensor(1.5248, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 21 \tLoss: tensor(1.5337, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 22 \tLoss: tensor(1.5238, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 23 \tLoss: tensor(1.5189, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 24 \tLoss: tensor(1.5431, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 25 \tLoss: tensor(1.5664, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 26 \tLoss: tensor(1.5136, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 27 \tLoss: tensor(1.5093, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 28 \tLoss: tensor(1.5326, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 29 \tLoss: tensor(1.5321, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 30 \tLoss: tensor(1.5478, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 31 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 32 \tLoss: tensor(1.5280, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 33 \tLoss: tensor(1.5134, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 34 \tLoss: tensor(1.5166, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 35 \tLoss: tensor(1.5276, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 36 \tLoss: tensor(1.5431, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 37 \tLoss: tensor(1.5124, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 38 \tLoss: tensor(1.5270, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 39 \tLoss: tensor(1.5299, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 40 \tLoss: tensor(1.5330, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 41 \tLoss: tensor(1.5285, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 42 \tLoss: tensor(1.5115, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 43 \tLoss: tensor(1.5429, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 44 \tLoss: tensor(1.5219, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 45 \tLoss: tensor(1.5474, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 46 \tLoss: tensor(1.5524, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 47 \tLoss: tensor(1.5251, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 48 \tLoss: tensor(1.5470, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 49 \tLoss: tensor(1.5154, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 50 \tLoss: tensor(1.5526, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 51 \tLoss: tensor(1.5119, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 52 \tLoss: tensor(1.4968, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 53 \tLoss: tensor(1.5263, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 54 \tLoss: tensor(1.5141, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 55 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 56 \tLoss: tensor(1.5121, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 57 \tLoss: tensor(1.5217, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 58 \tLoss: tensor(1.5219, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 59 \tLoss: tensor(1.5140, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 60 \tLoss: tensor(1.5113, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 61 \tLoss: tensor(1.5289, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 62 \tLoss: tensor(1.5139, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 63 \tLoss: tensor(1.5179, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 64 \tLoss: tensor(1.5190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 65 \tLoss: tensor(1.5217, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 66 \tLoss: tensor(1.5174, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 67 \tLoss: tensor(1.5099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 68 \tLoss: tensor(1.5131, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 69 \tLoss: tensor(1.4933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 70 \tLoss: tensor(1.5336, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 71 \tLoss: tensor(1.5382, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 72 \tLoss: tensor(1.5210, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 73 \tLoss: tensor(1.5185, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 74 \tLoss: tensor(1.5342, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 75 \tLoss: tensor(1.5322, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 76 \tLoss: tensor(1.5211, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 77 \tLoss: tensor(1.5029, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 78 \tLoss: tensor(1.5613, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 79 \tLoss: tensor(1.5202, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 80 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 81 \tLoss: tensor(1.5213, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 82 \tLoss: tensor(1.5323, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 83 \tLoss: tensor(1.5379, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 84 \tLoss: tensor(1.5241, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 85 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 86 \tLoss: tensor(1.5322, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 87 \tLoss: tensor(1.5454, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 88 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 89 \tLoss: tensor(1.5351, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 90 \tLoss: tensor(1.5282, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 91 \tLoss: tensor(1.5537, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 92 \tLoss: tensor(1.5410, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 93 \tLoss: tensor(1.5177, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 94 \tLoss: tensor(1.5370, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 95 \tLoss: tensor(1.5353, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 96 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 97 \tLoss: tensor(1.5093, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 98 \tLoss: tensor(1.5053, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 99 \tLoss: tensor(1.5259, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 100 \tLoss: tensor(1.5290, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 101 \tLoss: tensor(1.5280, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 102 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 103 \tLoss: tensor(1.5245, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 104 \tLoss: tensor(1.5263, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 105 \tLoss: tensor(1.5110, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 106 \tLoss: tensor(1.5240, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 107 \tLoss: tensor(1.5222, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 108 \tLoss: tensor(1.5306, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 109 \tLoss: tensor(1.5365, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 110 \tLoss: tensor(1.5261, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 111 \tLoss: tensor(1.5150, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 112 \tLoss: tensor(1.5257, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 113 \tLoss: tensor(1.5359, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 114 \tLoss: tensor(1.5221, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 115 \tLoss: tensor(1.5321, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 116 \tLoss: tensor(1.5323, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 117 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 118 \tLoss: tensor(1.5228, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 119 \tLoss: tensor(1.5185, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 120 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 121 \tLoss: tensor(1.5190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 122 \tLoss: tensor(1.5149, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 123 \tLoss: tensor(1.5068, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 124 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 125 \tLoss: tensor(1.5272, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 126 \tLoss: tensor(1.5448, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 127 \tLoss: tensor(1.5202, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 128 \tLoss: tensor(1.5241, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 129 \tLoss: tensor(1.5364, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 130 \tLoss: tensor(1.5283, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 131 \tLoss: tensor(1.5128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 132 \tLoss: tensor(1.5001, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 133 \tLoss: tensor(1.5118, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 134 \tLoss: tensor(1.5190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 135 \tLoss: tensor(1.5219, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 136 \tLoss: tensor(1.5160, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 137 \tLoss: tensor(1.5244, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 138 \tLoss: tensor(1.5032, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 139 \tLoss: tensor(1.5105, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 140 \tLoss: tensor(1.5227, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 141 \tLoss: tensor(1.4999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 142 \tLoss: tensor(1.5132, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 143 \tLoss: tensor(1.5608, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 144 \tLoss: tensor(1.5197, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 145 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 146 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 147 \tLoss: tensor(1.5009, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 148 \tLoss: tensor(1.5155, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 149 \tLoss: tensor(1.5209, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 150 \tLoss: tensor(1.5210, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 151 \tLoss: tensor(1.5195, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 152 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 153 \tLoss: tensor(1.5399, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 154 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 155 \tLoss: tensor(1.5194, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 156 \tLoss: tensor(1.5197, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 157 \tLoss: tensor(1.5276, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 158 \tLoss: tensor(1.5386, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 159 \tLoss: tensor(1.5129, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 160 \tLoss: tensor(1.5178, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 161 \tLoss: tensor(1.5267, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tBatch: 162 \tLoss: tensor(1.5257, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 163 \tLoss: tensor(1.5435, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 164 \tLoss: tensor(1.5206, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 165 \tLoss: tensor(1.5162, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 166 \tLoss: tensor(1.5319, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 167 \tLoss: tensor(1.5224, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 168 \tLoss: tensor(1.5104, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 169 \tLoss: tensor(1.5129, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 170 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 171 \tLoss: tensor(1.5234, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 172 \tLoss: tensor(1.4938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 173 \tLoss: tensor(1.5060, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 174 \tLoss: tensor(1.5090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 175 \tLoss: tensor(1.5266, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 176 \tLoss: tensor(1.5129, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 177 \tLoss: tensor(1.5178, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 178 \tLoss: tensor(1.5074, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 179 \tLoss: tensor(1.5116, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 180 \tLoss: tensor(1.5295, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 181 \tLoss: tensor(1.5123, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 182 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 183 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 184 \tLoss: tensor(1.5245, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 185 \tLoss: tensor(1.5067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 186 \tLoss: tensor(1.5198, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 187 \tLoss: tensor(1.4974, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 188 \tLoss: tensor(1.4986, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 189 \tLoss: tensor(1.5061, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 190 \tLoss: tensor(1.5103, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 191 \tLoss: tensor(1.5252, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 192 \tLoss: tensor(1.5290, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 193 \tLoss: tensor(1.5102, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 194 \tLoss: tensor(1.5259, grad_fn=<NllLossBackward>)\n",
      "Epoch: 1 \tBatch: 195 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 0 \tLoss: tensor(1.5126, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 1 \tLoss: tensor(1.5097, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 2 \tLoss: tensor(1.4879, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 3 \tLoss: tensor(1.5226, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 4 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 5 \tLoss: tensor(1.5032, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 6 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 7 \tLoss: tensor(1.5011, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 8 \tLoss: tensor(1.5044, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 9 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 10 \tLoss: tensor(1.5212, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 11 \tLoss: tensor(1.5266, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 12 \tLoss: tensor(1.5125, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 13 \tLoss: tensor(1.5101, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 14 \tLoss: tensor(1.5257, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 15 \tLoss: tensor(1.4968, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 16 \tLoss: tensor(1.5145, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 17 \tLoss: tensor(1.5116, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 18 \tLoss: tensor(1.5313, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 19 \tLoss: tensor(1.5038, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 20 \tLoss: tensor(1.4914, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 21 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 22 \tLoss: tensor(1.5050, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 23 \tLoss: tensor(1.5131, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 24 \tLoss: tensor(1.5011, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 25 \tLoss: tensor(1.5079, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 26 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 27 \tLoss: tensor(1.5049, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 28 \tLoss: tensor(1.5137, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 29 \tLoss: tensor(1.5346, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 30 \tLoss: tensor(1.5153, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 31 \tLoss: tensor(1.5137, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 32 \tLoss: tensor(1.5214, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 33 \tLoss: tensor(1.5037, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 34 \tLoss: tensor(1.5070, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 35 \tLoss: tensor(1.5085, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 36 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 37 \tLoss: tensor(1.4999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 38 \tLoss: tensor(1.5245, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 39 \tLoss: tensor(1.5105, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 40 \tLoss: tensor(1.5051, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 41 \tLoss: tensor(1.5222, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 42 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 43 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 44 \tLoss: tensor(1.5134, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 45 \tLoss: tensor(1.5129, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 46 \tLoss: tensor(1.5172, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 47 \tLoss: tensor(1.5169, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 48 \tLoss: tensor(1.5068, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 49 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 50 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 51 \tLoss: tensor(1.5291, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 52 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 53 \tLoss: tensor(1.5098, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 54 \tLoss: tensor(1.5061, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 55 \tLoss: tensor(1.5064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 56 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 57 \tLoss: tensor(1.5182, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 58 \tLoss: tensor(1.5148, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 59 \tLoss: tensor(1.5076, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 60 \tLoss: tensor(1.4949, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 61 \tLoss: tensor(1.5077, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 62 \tLoss: tensor(1.5168, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 63 \tLoss: tensor(1.4995, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 64 \tLoss: tensor(1.5193, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 65 \tLoss: tensor(1.5096, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 66 \tLoss: tensor(1.5445, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 67 \tLoss: tensor(1.5108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 68 \tLoss: tensor(1.5209, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 69 \tLoss: tensor(1.5081, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 70 \tLoss: tensor(1.4931, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 71 \tLoss: tensor(1.5035, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 72 \tLoss: tensor(1.5103, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 73 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 74 \tLoss: tensor(1.5127, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 75 \tLoss: tensor(1.5010, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 76 \tLoss: tensor(1.5092, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 77 \tLoss: tensor(1.4981, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 78 \tLoss: tensor(1.4992, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 79 \tLoss: tensor(1.4933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 80 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 81 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 82 \tLoss: tensor(1.5025, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 83 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 84 \tLoss: tensor(1.5049, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2 \tBatch: 85 \tLoss: tensor(1.4914, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 86 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 87 \tLoss: tensor(1.4998, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 88 \tLoss: tensor(1.5076, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 89 \tLoss: tensor(1.5083, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 90 \tLoss: tensor(1.4920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 91 \tLoss: tensor(1.4988, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 92 \tLoss: tensor(1.5230, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 93 \tLoss: tensor(1.4933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 94 \tLoss: tensor(1.5375, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 95 \tLoss: tensor(1.5138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 96 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 97 \tLoss: tensor(1.5127, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 98 \tLoss: tensor(1.5087, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 99 \tLoss: tensor(1.5119, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 100 \tLoss: tensor(1.4980, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 101 \tLoss: tensor(1.5111, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 102 \tLoss: tensor(1.4984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 103 \tLoss: tensor(1.5020, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 104 \tLoss: tensor(1.5029, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 105 \tLoss: tensor(1.5304, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 106 \tLoss: tensor(1.5274, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 107 \tLoss: tensor(1.5138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 108 \tLoss: tensor(1.5236, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 109 \tLoss: tensor(1.5028, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 110 \tLoss: tensor(1.5203, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 111 \tLoss: tensor(1.5216, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 112 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 113 \tLoss: tensor(1.5004, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 114 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 115 \tLoss: tensor(1.5264, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 116 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 117 \tLoss: tensor(1.5272, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 118 \tLoss: tensor(1.5202, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 119 \tLoss: tensor(1.5114, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 120 \tLoss: tensor(1.4899, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 121 \tLoss: tensor(1.5159, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 122 \tLoss: tensor(1.5109, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 123 \tLoss: tensor(1.5009, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 124 \tLoss: tensor(1.5075, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 125 \tLoss: tensor(1.4968, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 126 \tLoss: tensor(1.5175, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 127 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 128 \tLoss: tensor(1.5154, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 129 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 130 \tLoss: tensor(1.5101, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 131 \tLoss: tensor(1.4895, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 132 \tLoss: tensor(1.5138, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 133 \tLoss: tensor(1.5046, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 134 \tLoss: tensor(1.5128, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 135 \tLoss: tensor(1.5078, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 136 \tLoss: tensor(1.4957, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 137 \tLoss: tensor(1.5099, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 138 \tLoss: tensor(1.5030, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 139 \tLoss: tensor(1.5119, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 140 \tLoss: tensor(1.5009, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 141 \tLoss: tensor(1.5071, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 142 \tLoss: tensor(1.4986, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 143 \tLoss: tensor(1.5221, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 144 \tLoss: tensor(1.4963, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 145 \tLoss: tensor(1.5023, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 146 \tLoss: tensor(1.5101, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 147 \tLoss: tensor(1.5114, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 148 \tLoss: tensor(1.5258, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 149 \tLoss: tensor(1.4985, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 150 \tLoss: tensor(1.5013, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 151 \tLoss: tensor(1.4957, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 152 \tLoss: tensor(1.5027, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 153 \tLoss: tensor(1.4996, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 154 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 155 \tLoss: tensor(1.5089, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 156 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 157 \tLoss: tensor(1.5074, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 158 \tLoss: tensor(1.4997, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 159 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 160 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 161 \tLoss: tensor(1.5046, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 162 \tLoss: tensor(1.5269, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 163 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 164 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 165 \tLoss: tensor(1.4995, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 166 \tLoss: tensor(1.4916, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 167 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 168 \tLoss: tensor(1.5019, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 169 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 170 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 171 \tLoss: tensor(1.5189, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 172 \tLoss: tensor(1.4808, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 173 \tLoss: tensor(1.4922, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 174 \tLoss: tensor(1.5007, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 175 \tLoss: tensor(1.5160, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 176 \tLoss: tensor(1.5069, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 177 \tLoss: tensor(1.4984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 178 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 179 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 180 \tLoss: tensor(1.5027, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 181 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 182 \tLoss: tensor(1.4964, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 183 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 184 \tLoss: tensor(1.5219, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 185 \tLoss: tensor(1.5010, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 186 \tLoss: tensor(1.4895, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 187 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 188 \tLoss: tensor(1.5157, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 189 \tLoss: tensor(1.5190, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 190 \tLoss: tensor(1.5032, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 191 \tLoss: tensor(1.5033, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 192 \tLoss: tensor(1.4810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 193 \tLoss: tensor(1.5034, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 194 \tLoss: tensor(1.4979, grad_fn=<NllLossBackward>)\n",
      "Epoch: 2 \tBatch: 195 \tLoss: tensor(1.4902, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 0 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 1 \tLoss: tensor(1.5100, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 2 \tLoss: tensor(1.5111, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 3 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 4 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 5 \tLoss: tensor(1.5031, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 6 \tLoss: tensor(1.4822, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 7 \tLoss: tensor(1.5014, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 8 \tLoss: tensor(1.4933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 9 \tLoss: tensor(1.4951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 10 \tLoss: tensor(1.5230, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 11 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 12 \tLoss: tensor(1.4943, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 13 \tLoss: tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 14 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 15 \tLoss: tensor(1.4991, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 16 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 17 \tLoss: tensor(1.5024, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 18 \tLoss: tensor(1.4891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 19 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 20 \tLoss: tensor(1.5016, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 21 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 22 \tLoss: tensor(1.5063, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 23 \tLoss: tensor(1.4931, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 24 \tLoss: tensor(1.5040, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 25 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 26 \tLoss: tensor(1.4954, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 27 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 28 \tLoss: tensor(1.4991, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 29 \tLoss: tensor(1.5044, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 30 \tLoss: tensor(1.5131, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 31 \tLoss: tensor(1.4972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 32 \tLoss: tensor(1.5067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 33 \tLoss: tensor(1.4761, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 34 \tLoss: tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 35 \tLoss: tensor(1.5073, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 36 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 37 \tLoss: tensor(1.5014, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 38 \tLoss: tensor(1.5108, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 39 \tLoss: tensor(1.5103, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 40 \tLoss: tensor(1.4898, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 41 \tLoss: tensor(1.4953, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 42 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 43 \tLoss: tensor(1.4902, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 44 \tLoss: tensor(1.5073, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 45 \tLoss: tensor(1.4876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 46 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 47 \tLoss: tensor(1.5117, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 48 \tLoss: tensor(1.4979, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 49 \tLoss: tensor(1.5240, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 50 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 51 \tLoss: tensor(1.5114, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 52 \tLoss: tensor(1.4751, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 53 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 54 \tLoss: tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 55 \tLoss: tensor(1.5090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 56 \tLoss: tensor(1.4999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 57 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 58 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 59 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 60 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 61 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 62 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 63 \tLoss: tensor(1.5001, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 64 \tLoss: tensor(1.4812, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 65 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 66 \tLoss: tensor(1.4987, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 67 \tLoss: tensor(1.5043, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 68 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 69 \tLoss: tensor(1.5096, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 70 \tLoss: tensor(1.5062, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 71 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 72 \tLoss: tensor(1.5036, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 73 \tLoss: tensor(1.4923, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 74 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 75 \tLoss: tensor(1.5079, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 76 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 77 \tLoss: tensor(1.5130, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 78 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 79 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 80 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 81 \tLoss: tensor(1.4995, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 82 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 83 \tLoss: tensor(1.4985, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 84 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 85 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 86 \tLoss: tensor(1.4922, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 87 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 88 \tLoss: tensor(1.4832, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 89 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 90 \tLoss: tensor(1.5152, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 91 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 92 \tLoss: tensor(1.4875, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 93 \tLoss: tensor(1.4878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 94 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 95 \tLoss: tensor(1.4888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 96 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 97 \tLoss: tensor(1.4949, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 98 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 99 \tLoss: tensor(1.5060, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 100 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 101 \tLoss: tensor(1.4865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 102 \tLoss: tensor(1.5055, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 103 \tLoss: tensor(1.4993, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 104 \tLoss: tensor(1.4994, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 105 \tLoss: tensor(1.4783, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 106 \tLoss: tensor(1.5141, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 107 \tLoss: tensor(1.4983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 108 \tLoss: tensor(1.5098, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 109 \tLoss: tensor(1.4986, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 110 \tLoss: tensor(1.5291, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 111 \tLoss: tensor(1.4696, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 112 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 113 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 114 \tLoss: tensor(1.5072, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 115 \tLoss: tensor(1.5009, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 116 \tLoss: tensor(1.5016, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 117 \tLoss: tensor(1.5076, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 118 \tLoss: tensor(1.5022, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 119 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 120 \tLoss: tensor(1.4979, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 121 \tLoss: tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 122 \tLoss: tensor(1.4967, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 123 \tLoss: tensor(1.5208, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 124 \tLoss: tensor(1.4968, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 125 \tLoss: tensor(1.4984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 126 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3 \tBatch: 127 \tLoss: tensor(1.5095, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 128 \tLoss: tensor(1.4894, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 129 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 130 \tLoss: tensor(1.4967, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 131 \tLoss: tensor(1.4827, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 132 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 133 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 134 \tLoss: tensor(1.4977, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 135 \tLoss: tensor(1.5007, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 136 \tLoss: tensor(1.4956, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 137 \tLoss: tensor(1.4956, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 138 \tLoss: tensor(1.4947, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 139 \tLoss: tensor(1.5107, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 140 \tLoss: tensor(1.5104, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 141 \tLoss: tensor(1.5095, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 142 \tLoss: tensor(1.5066, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 143 \tLoss: tensor(1.5060, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 144 \tLoss: tensor(1.4989, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 145 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 146 \tLoss: tensor(1.5011, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 147 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 148 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 149 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 150 \tLoss: tensor(1.5054, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 151 \tLoss: tensor(1.4864, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 152 \tLoss: tensor(1.4987, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 153 \tLoss: tensor(1.5022, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 154 \tLoss: tensor(1.4914, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 155 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 156 \tLoss: tensor(1.4985, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 157 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 158 \tLoss: tensor(1.5067, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 159 \tLoss: tensor(1.5189, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 160 \tLoss: tensor(1.4898, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 161 \tLoss: tensor(1.4964, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 162 \tLoss: tensor(1.5062, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 163 \tLoss: tensor(1.5094, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 164 \tLoss: tensor(1.4863, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 165 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 166 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 167 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 168 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 169 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 170 \tLoss: tensor(1.4916, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 171 \tLoss: tensor(1.4913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 172 \tLoss: tensor(1.5004, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 173 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 174 \tLoss: tensor(1.4895, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 175 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 176 \tLoss: tensor(1.4876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 177 \tLoss: tensor(1.5097, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 178 \tLoss: tensor(1.5033, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 179 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 180 \tLoss: tensor(1.4955, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 181 \tLoss: tensor(1.4941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 182 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 183 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 184 \tLoss: tensor(1.5228, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 185 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 186 \tLoss: tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 187 \tLoss: tensor(1.4913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 188 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 189 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 190 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 191 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 192 \tLoss: tensor(1.4899, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 193 \tLoss: tensor(1.5018, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 194 \tLoss: tensor(1.5174, grad_fn=<NllLossBackward>)\n",
      "Epoch: 3 \tBatch: 195 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 0 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 1 \tLoss: tensor(1.4984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 2 \tLoss: tensor(1.5136, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 3 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 4 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 5 \tLoss: tensor(1.4745, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 6 \tLoss: tensor(1.4878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 7 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 8 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 9 \tLoss: tensor(1.4801, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 10 \tLoss: tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 11 \tLoss: tensor(1.5090, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 12 \tLoss: tensor(1.5011, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 13 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 14 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 15 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 16 \tLoss: tensor(1.4870, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 17 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 18 \tLoss: tensor(1.4983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 19 \tLoss: tensor(1.4805, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 20 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 21 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 22 \tLoss: tensor(1.4991, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 23 \tLoss: tensor(1.5096, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 24 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 25 \tLoss: tensor(1.4950, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 26 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 27 \tLoss: tensor(1.5000, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 28 \tLoss: tensor(1.4919, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 29 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 30 \tLoss: tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 31 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 32 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 33 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 34 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 35 \tLoss: tensor(1.4878, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 36 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 37 \tLoss: tensor(1.4902, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 38 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 39 \tLoss: tensor(1.5178, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 40 \tLoss: tensor(1.5078, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 41 \tLoss: tensor(1.5015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 42 \tLoss: tensor(1.5052, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 43 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 44 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 45 \tLoss: tensor(1.4813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 46 \tLoss: tensor(1.4863, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 47 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 48 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 49 \tLoss: tensor(1.4927, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 50 \tLoss: tensor(1.4891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 51 \tLoss: tensor(1.4861, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 52 \tLoss: tensor(1.4813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 53 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 54 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 55 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 56 \tLoss: tensor(1.4846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 57 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 58 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 59 \tLoss: tensor(1.4811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 60 \tLoss: tensor(1.5022, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 61 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 62 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 63 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 64 \tLoss: tensor(1.4972, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 65 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 66 \tLoss: tensor(1.4971, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 67 \tLoss: tensor(1.5207, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 68 \tLoss: tensor(1.4782, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 69 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 70 \tLoss: tensor(1.4827, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 71 \tLoss: tensor(1.5047, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 72 \tLoss: tensor(1.4756, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 73 \tLoss: tensor(1.4941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 74 \tLoss: tensor(1.5080, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 75 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 76 \tLoss: tensor(1.4995, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 77 \tLoss: tensor(1.5019, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 78 \tLoss: tensor(1.5051, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 79 \tLoss: tensor(1.4960, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 80 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 81 \tLoss: tensor(1.4922, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 82 \tLoss: tensor(1.4820, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 83 \tLoss: tensor(1.5059, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 84 \tLoss: tensor(1.4821, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 85 \tLoss: tensor(1.4888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 86 \tLoss: tensor(1.5048, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 87 \tLoss: tensor(1.5093, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 88 \tLoss: tensor(1.4891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 89 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 90 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 91 \tLoss: tensor(1.4899, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 92 \tLoss: tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 93 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 94 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 95 \tLoss: tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 96 \tLoss: tensor(1.4874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 97 \tLoss: tensor(1.4753, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 98 \tLoss: tensor(1.4951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 99 \tLoss: tensor(1.4969, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 100 \tLoss: tensor(1.4984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 101 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 102 \tLoss: tensor(1.4933, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 103 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 104 \tLoss: tensor(1.4994, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 105 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 106 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 107 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 108 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 109 \tLoss: tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 110 \tLoss: tensor(1.4727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 111 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 112 \tLoss: tensor(1.4876, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 113 \tLoss: tensor(1.4832, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 114 \tLoss: tensor(1.4936, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 115 \tLoss: tensor(1.5070, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 116 \tLoss: tensor(1.4951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 117 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 118 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 119 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 120 \tLoss: tensor(1.5073, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 121 \tLoss: tensor(1.4920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 122 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 123 \tLoss: tensor(1.4748, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 124 \tLoss: tensor(1.4909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 125 \tLoss: tensor(1.4863, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 126 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 127 \tLoss: tensor(1.4885, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 128 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 129 \tLoss: tensor(1.4959, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 130 \tLoss: tensor(1.4944, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 131 \tLoss: tensor(1.4805, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 132 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 133 \tLoss: tensor(1.4969, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 134 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 135 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 136 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 137 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 138 \tLoss: tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 139 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 140 \tLoss: tensor(1.5148, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 141 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 142 \tLoss: tensor(1.4985, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 143 \tLoss: tensor(1.4932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 144 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 145 \tLoss: tensor(1.5073, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 146 \tLoss: tensor(1.4821, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 147 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 148 \tLoss: tensor(1.5124, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 149 \tLoss: tensor(1.4966, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 150 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 151 \tLoss: tensor(1.5012, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 152 \tLoss: tensor(1.4789, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 153 \tLoss: tensor(1.4752, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 154 \tLoss: tensor(1.5040, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 155 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 156 \tLoss: tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 157 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 158 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 159 \tLoss: tensor(1.5015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 160 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 161 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 162 \tLoss: tensor(1.4817, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 163 \tLoss: tensor(1.5033, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 164 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 165 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 166 \tLoss: tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 167 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \tBatch: 168 \tLoss: tensor(1.4879, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 169 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 170 \tLoss: tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 171 \tLoss: tensor(1.5069, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 172 \tLoss: tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 173 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 174 \tLoss: tensor(1.4811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 175 \tLoss: tensor(1.4675, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 176 \tLoss: tensor(1.5061, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 177 \tLoss: tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 178 \tLoss: tensor(1.4975, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 179 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 180 \tLoss: tensor(1.4924, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 181 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 182 \tLoss: tensor(1.4935, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 183 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 184 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 185 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 186 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 187 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 188 \tLoss: tensor(1.4856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 189 \tLoss: tensor(1.4884, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 190 \tLoss: tensor(1.4936, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 191 \tLoss: tensor(1.4663, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 192 \tLoss: tensor(1.5031, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 193 \tLoss: tensor(1.5017, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 194 \tLoss: tensor(1.4976, grad_fn=<NllLossBackward>)\n",
      "Epoch: 4 \tBatch: 195 \tLoss: tensor(1.4999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 0 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 1 \tLoss: tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 2 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 3 \tLoss: tensor(1.5182, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 4 \tLoss: tensor(1.4822, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 5 \tLoss: tensor(1.4911, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 6 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 7 \tLoss: tensor(1.4949, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 8 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 9 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 10 \tLoss: tensor(1.4752, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 11 \tLoss: tensor(1.5027, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 12 \tLoss: tensor(1.4894, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 13 \tLoss: tensor(1.4694, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 14 \tLoss: tensor(1.4845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 15 \tLoss: tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 16 \tLoss: tensor(1.4864, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 17 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 18 \tLoss: tensor(1.4901, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 19 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 20 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 21 \tLoss: tensor(1.4926, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 22 \tLoss: tensor(1.4920, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 23 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 24 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 25 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 26 \tLoss: tensor(1.5015, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 27 \tLoss: tensor(1.4947, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 28 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 29 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 30 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 31 \tLoss: tensor(1.4752, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 32 \tLoss: tensor(1.4983, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 33 \tLoss: tensor(1.4813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 34 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 35 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 36 \tLoss: tensor(1.4950, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 37 \tLoss: tensor(1.4814, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 38 \tLoss: tensor(1.4758, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 39 \tLoss: tensor(1.4759, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 40 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 41 \tLoss: tensor(1.4999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 42 \tLoss: tensor(1.4732, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 43 \tLoss: tensor(1.4937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 44 \tLoss: tensor(1.4999, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 45 \tLoss: tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 46 \tLoss: tensor(1.4894, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 47 \tLoss: tensor(1.4712, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 48 \tLoss: tensor(1.4902, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 49 \tLoss: tensor(1.4733, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 50 \tLoss: tensor(1.4848, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 51 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 52 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 53 \tLoss: tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 54 \tLoss: tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 55 \tLoss: tensor(1.5010, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 56 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 57 \tLoss: tensor(1.4913, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 58 \tLoss: tensor(1.4919, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 59 \tLoss: tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 60 \tLoss: tensor(1.4846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 61 \tLoss: tensor(1.4902, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 62 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 63 \tLoss: tensor(1.4667, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 64 \tLoss: tensor(1.4730, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 65 \tLoss: tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 66 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 67 \tLoss: tensor(1.4925, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 68 \tLoss: tensor(1.4725, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 69 \tLoss: tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 70 \tLoss: tensor(1.4856, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 71 \tLoss: tensor(1.4826, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 72 \tLoss: tensor(1.4899, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 73 \tLoss: tensor(1.4966, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 74 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 75 \tLoss: tensor(1.4866, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 76 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 77 \tLoss: tensor(1.4814, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 78 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 79 \tLoss: tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 80 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 81 \tLoss: tensor(1.4891, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 82 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 83 \tLoss: tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 84 \tLoss: tensor(1.4951, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 85 \tLoss: tensor(1.4991, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 86 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 87 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 88 \tLoss: tensor(1.4851, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 89 \tLoss: tensor(1.4879, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 90 \tLoss: tensor(1.4848, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 91 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 92 \tLoss: tensor(1.4977, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5 \tBatch: 93 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 94 \tLoss: tensor(1.4985, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 95 \tLoss: tensor(1.4996, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 96 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 97 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 98 \tLoss: tensor(1.4753, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 99 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 100 \tLoss: tensor(1.4973, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 101 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 102 \tLoss: tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 103 \tLoss: tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 104 \tLoss: tensor(1.4731, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 105 \tLoss: tensor(1.4994, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 106 \tLoss: tensor(1.4870, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 107 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 108 \tLoss: tensor(1.5064, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 109 \tLoss: tensor(1.4909, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 110 \tLoss: tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 111 \tLoss: tensor(1.4822, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 112 \tLoss: tensor(1.4814, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 113 \tLoss: tensor(1.4674, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 114 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 115 \tLoss: tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 116 \tLoss: tensor(1.4739, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 117 \tLoss: tensor(1.4805, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 118 \tLoss: tensor(1.4780, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 119 \tLoss: tensor(1.4729, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 120 \tLoss: tensor(1.4844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 121 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 122 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 123 \tLoss: tensor(1.4817, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 124 \tLoss: tensor(1.4726, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 125 \tLoss: tensor(1.4810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 126 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 127 \tLoss: tensor(1.4714, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 128 \tLoss: tensor(1.4831, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 129 \tLoss: tensor(1.5036, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 130 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 131 \tLoss: tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 132 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 133 \tLoss: tensor(1.4751, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 134 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 135 \tLoss: tensor(1.4848, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 136 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 137 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 138 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 139 \tLoss: tensor(1.4735, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 140 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 141 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 142 \tLoss: tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 143 \tLoss: tensor(1.4990, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 144 \tLoss: tensor(1.4879, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 145 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 146 \tLoss: tensor(1.4824, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 147 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 148 \tLoss: tensor(1.4842, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 149 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 150 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 151 \tLoss: tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 152 \tLoss: tensor(1.5006, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 153 \tLoss: tensor(1.4967, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 154 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 155 \tLoss: tensor(1.4938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 156 \tLoss: tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 157 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 158 \tLoss: tensor(1.4887, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 159 \tLoss: tensor(1.5038, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 160 \tLoss: tensor(1.4708, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 161 \tLoss: tensor(1.4917, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 162 \tLoss: tensor(1.5059, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 163 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 164 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 165 \tLoss: tensor(1.4941, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 166 \tLoss: tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 167 \tLoss: tensor(1.4938, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 168 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 169 \tLoss: tensor(1.4988, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 170 \tLoss: tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 171 \tLoss: tensor(1.4883, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 172 \tLoss: tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 173 \tLoss: tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 174 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 175 \tLoss: tensor(1.4970, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 176 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 177 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 178 \tLoss: tensor(1.4749, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 179 \tLoss: tensor(1.4832, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 180 \tLoss: tensor(1.4768, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 181 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 182 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 183 \tLoss: tensor(1.4916, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 184 \tLoss: tensor(1.4927, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 185 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 186 \tLoss: tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 187 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 188 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 189 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 190 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 191 \tLoss: tensor(1.4688, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 192 \tLoss: tensor(1.4939, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 193 \tLoss: tensor(1.4617, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 194 \tLoss: tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "Epoch: 5 \tBatch: 195 \tLoss: tensor(1.5253, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 0 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 1 \tLoss: tensor(1.5028, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 2 \tLoss: tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 3 \tLoss: tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 4 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 5 \tLoss: tensor(1.4737, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 6 \tLoss: tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 7 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 8 \tLoss: tensor(1.4829, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 9 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 10 \tLoss: tensor(1.5005, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 11 \tLoss: tensor(1.4847, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 12 \tLoss: tensor(1.4954, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 13 \tLoss: tensor(1.4873, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 14 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tBatch: 15 \tLoss: tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 16 \tLoss: tensor(1.4905, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 17 \tLoss: tensor(1.4904, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 18 \tLoss: tensor(1.4760, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 19 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 20 \tLoss: tensor(1.4911, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 21 \tLoss: tensor(1.4952, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 22 \tLoss: tensor(1.5033, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 23 \tLoss: tensor(1.4831, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 24 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 25 \tLoss: tensor(1.4863, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 26 \tLoss: tensor(1.4714, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 27 \tLoss: tensor(1.4759, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 28 \tLoss: tensor(1.4753, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 29 \tLoss: tensor(1.4675, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 30 \tLoss: tensor(1.4984, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 31 \tLoss: tensor(1.4744, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 32 \tLoss: tensor(1.4727, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 33 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 34 \tLoss: tensor(1.4810, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 35 \tLoss: tensor(1.4907, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 36 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 37 \tLoss: tensor(1.4855, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 38 \tLoss: tensor(1.4936, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 39 \tLoss: tensor(1.4886, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 40 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 41 \tLoss: tensor(1.4895, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 42 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 43 \tLoss: tensor(1.4782, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 44 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 45 \tLoss: tensor(1.4844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 46 \tLoss: tensor(1.4674, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 47 \tLoss: tensor(1.4756, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 48 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 49 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 50 \tLoss: tensor(1.4660, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 51 \tLoss: tensor(1.4811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 52 \tLoss: tensor(1.4710, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 53 \tLoss: tensor(1.4752, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 54 \tLoss: tensor(1.4732, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 55 \tLoss: tensor(1.4991, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 56 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 57 \tLoss: tensor(1.4848, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 58 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 59 \tLoss: tensor(1.4632, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 60 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 61 \tLoss: tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 62 \tLoss: tensor(1.4743, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 63 \tLoss: tensor(1.4874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 64 \tLoss: tensor(1.4827, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 65 \tLoss: tensor(1.4768, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 66 \tLoss: tensor(1.4897, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 67 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 68 \tLoss: tensor(1.4958, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 69 \tLoss: tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 70 \tLoss: tensor(1.4934, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 71 \tLoss: tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 72 \tLoss: tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 73 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 74 \tLoss: tensor(1.4823, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 75 \tLoss: tensor(1.4735, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 76 \tLoss: tensor(1.4819, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 77 \tLoss: tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 78 \tLoss: tensor(1.4750, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 79 \tLoss: tensor(1.4735, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 80 \tLoss: tensor(1.4794, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 81 \tLoss: tensor(1.4945, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 82 \tLoss: tensor(1.4917, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 83 \tLoss: tensor(1.4868, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 84 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 85 \tLoss: tensor(1.4731, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 86 \tLoss: tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 87 \tLoss: tensor(1.4818, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 88 \tLoss: tensor(1.4979, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 89 \tLoss: tensor(1.4783, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 90 \tLoss: tensor(1.4846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 91 \tLoss: tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 92 \tLoss: tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 93 \tLoss: tensor(1.4808, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 94 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 95 \tLoss: tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 96 \tLoss: tensor(1.4677, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 97 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 98 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 99 \tLoss: tensor(1.4867, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 100 \tLoss: tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 101 \tLoss: tensor(1.4839, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 102 \tLoss: tensor(1.4813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 103 \tLoss: tensor(1.4932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 104 \tLoss: tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 105 \tLoss: tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 106 \tLoss: tensor(1.4781, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 107 \tLoss: tensor(1.4670, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 108 \tLoss: tensor(1.4678, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 109 \tLoss: tensor(1.4851, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 110 \tLoss: tensor(1.4697, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 111 \tLoss: tensor(1.4789, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 112 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 113 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 114 \tLoss: tensor(1.4888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 115 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 116 \tLoss: tensor(1.4809, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 117 \tLoss: tensor(1.4937, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 118 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 119 \tLoss: tensor(1.4848, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 120 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 121 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 122 \tLoss: tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 123 \tLoss: tensor(1.4768, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 124 \tLoss: tensor(1.4702, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 125 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 126 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 127 \tLoss: tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 128 \tLoss: tensor(1.4650, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 129 \tLoss: tensor(1.5008, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 130 \tLoss: tensor(1.4743, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 131 \tLoss: tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 132 \tLoss: tensor(1.5036, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 133 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 134 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6 \tBatch: 135 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 136 \tLoss: tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 137 \tLoss: tensor(1.4669, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 138 \tLoss: tensor(1.4916, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 139 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 140 \tLoss: tensor(1.4722, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 141 \tLoss: tensor(1.4690, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 142 \tLoss: tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 143 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 144 \tLoss: tensor(1.4783, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 145 \tLoss: tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 146 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 147 \tLoss: tensor(1.4788, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 148 \tLoss: tensor(1.4690, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 149 \tLoss: tensor(1.4838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 150 \tLoss: tensor(1.4725, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 151 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 152 \tLoss: tensor(1.4817, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 153 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 154 \tLoss: tensor(1.4936, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 155 \tLoss: tensor(1.4852, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 156 \tLoss: tensor(1.4950, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 157 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 158 \tLoss: tensor(1.4836, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 159 \tLoss: tensor(1.4697, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 160 \tLoss: tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 161 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 162 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 163 \tLoss: tensor(1.4712, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 164 \tLoss: tensor(1.4871, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 165 \tLoss: tensor(1.4860, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 166 \tLoss: tensor(1.4737, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 167 \tLoss: tensor(1.4805, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 168 \tLoss: tensor(1.4845, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 169 \tLoss: tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 170 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 171 \tLoss: tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 172 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 173 \tLoss: tensor(1.4957, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 174 \tLoss: tensor(1.4838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 175 \tLoss: tensor(1.4882, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 176 \tLoss: tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 177 \tLoss: tensor(1.4703, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 178 \tLoss: tensor(1.4821, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 179 \tLoss: tensor(1.4857, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 180 \tLoss: tensor(1.4946, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 181 \tLoss: tensor(1.4797, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 182 \tLoss: tensor(1.4846, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 183 \tLoss: tensor(1.4736, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 184 \tLoss: tensor(1.4865, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 185 \tLoss: tensor(1.4820, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 186 \tLoss: tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 187 \tLoss: tensor(1.4926, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 188 \tLoss: tensor(1.4928, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 189 \tLoss: tensor(1.4912, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 190 \tLoss: tensor(1.4932, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 191 \tLoss: tensor(1.4744, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 192 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 193 \tLoss: tensor(1.4641, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 194 \tLoss: tensor(1.4701, grad_fn=<NllLossBackward>)\n",
      "Epoch: 6 \tBatch: 195 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 0 \tLoss: tensor(1.4785, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 1 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 2 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 3 \tLoss: tensor(1.4665, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 4 \tLoss: tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 5 \tLoss: tensor(1.4947, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 6 \tLoss: tensor(1.4862, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 7 \tLoss: tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 8 \tLoss: tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 9 \tLoss: tensor(1.4681, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 10 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 11 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 12 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 13 \tLoss: tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 14 \tLoss: tensor(1.4756, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 15 \tLoss: tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 16 \tLoss: tensor(1.4680, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 17 \tLoss: tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 18 \tLoss: tensor(1.4759, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 19 \tLoss: tensor(1.4771, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 20 \tLoss: tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 21 \tLoss: tensor(1.4636, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 22 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 23 \tLoss: tensor(1.4793, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 24 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 25 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 26 \tLoss: tensor(1.4798, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 27 \tLoss: tensor(1.4716, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 28 \tLoss: tensor(1.4678, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 29 \tLoss: tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 30 \tLoss: tensor(1.4892, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 31 \tLoss: tensor(1.4803, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 32 \tLoss: tensor(1.4965, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 33 \tLoss: tensor(1.4877, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 34 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 35 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 36 \tLoss: tensor(1.4693, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 37 \tLoss: tensor(1.4833, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 38 \tLoss: tensor(1.4695, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 39 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 40 \tLoss: tensor(1.4706, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 41 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 42 \tLoss: tensor(1.4858, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 43 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 44 \tLoss: tensor(1.4633, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 45 \tLoss: tensor(1.4680, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 46 \tLoss: tensor(1.4961, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 47 \tLoss: tensor(1.4708, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 48 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 49 \tLoss: tensor(1.4800, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 50 \tLoss: tensor(1.4807, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 51 \tLoss: tensor(1.4720, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 52 \tLoss: tensor(1.4713, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 53 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 54 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 55 \tLoss: tensor(1.4766, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 56 \tLoss: tensor(1.4790, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tBatch: 57 \tLoss: tensor(1.4668, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 58 \tLoss: tensor(1.4859, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 59 \tLoss: tensor(1.4662, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 60 \tLoss: tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 61 \tLoss: tensor(1.4655, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 62 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 63 \tLoss: tensor(1.4730, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 64 \tLoss: tensor(1.4838, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 65 \tLoss: tensor(1.4844, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 66 \tLoss: tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 67 \tLoss: tensor(1.4772, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 68 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 69 \tLoss: tensor(1.4849, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 70 \tLoss: tensor(1.4726, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 71 \tLoss: tensor(1.4758, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 72 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 73 \tLoss: tensor(1.4762, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 74 \tLoss: tensor(1.4900, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 75 \tLoss: tensor(1.4799, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 76 \tLoss: tensor(1.4761, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 77 \tLoss: tensor(1.4896, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 78 \tLoss: tensor(1.4778, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 79 \tLoss: tensor(1.4943, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 80 \tLoss: tensor(1.4740, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 81 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 82 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 83 \tLoss: tensor(1.4842, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 84 \tLoss: tensor(1.4947, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 85 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 86 \tLoss: tensor(1.4915, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 87 \tLoss: tensor(1.4821, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 88 \tLoss: tensor(1.4815, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 89 \tLoss: tensor(1.4782, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 90 \tLoss: tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 91 \tLoss: tensor(1.4746, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 92 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 93 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 94 \tLoss: tensor(1.4805, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 95 \tLoss: tensor(1.4813, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 96 \tLoss: tensor(1.4764, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 97 \tLoss: tensor(1.4890, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 98 \tLoss: tensor(1.4929, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 99 \tLoss: tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 100 \tLoss: tensor(1.4917, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 101 \tLoss: tensor(1.4880, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 102 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 103 \tLoss: tensor(1.4679, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 104 \tLoss: tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 105 \tLoss: tensor(1.4830, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 106 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 107 \tLoss: tensor(1.4792, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 108 \tLoss: tensor(1.4910, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 109 \tLoss: tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 110 \tLoss: tensor(1.4748, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 111 \tLoss: tensor(1.4659, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 112 \tLoss: tensor(1.4784, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 113 \tLoss: tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 114 \tLoss: tensor(1.4787, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 115 \tLoss: tensor(1.4777, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 116 \tLoss: tensor(1.4686, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 117 \tLoss: tensor(1.4841, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 118 \tLoss: tensor(1.4631, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 119 \tLoss: tensor(1.4817, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 120 \tLoss: tensor(1.4758, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 121 \tLoss: tensor(1.4664, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 122 \tLoss: tensor(1.4759, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 123 \tLoss: tensor(1.4743, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 124 \tLoss: tensor(1.4828, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 125 \tLoss: tensor(1.4770, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 126 \tLoss: tensor(1.4752, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 127 \tLoss: tensor(1.4737, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 128 \tLoss: tensor(1.4942, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 129 \tLoss: tensor(1.4717, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 130 \tLoss: tensor(1.4825, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 131 \tLoss: tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 132 \tLoss: tensor(1.4753, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 133 \tLoss: tensor(1.4722, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 134 \tLoss: tensor(1.4811, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 135 \tLoss: tensor(1.4791, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 136 \tLoss: tensor(1.4888, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 137 \tLoss: tensor(1.5092, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 138 \tLoss: tensor(1.4930, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 139 \tLoss: tensor(1.4908, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 140 \tLoss: tensor(1.4816, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 141 \tLoss: tensor(1.4731, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 142 \tLoss: tensor(1.4786, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 143 \tLoss: tensor(1.4687, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 144 \tLoss: tensor(1.4709, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 145 \tLoss: tensor(1.4721, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 146 \tLoss: tensor(1.4773, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 147 \tLoss: tensor(1.4889, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 148 \tLoss: tensor(1.4775, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 149 \tLoss: tensor(1.4786, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 150 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 151 \tLoss: tensor(1.4700, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 152 \tLoss: tensor(1.4761, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 153 \tLoss: tensor(1.4906, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 154 \tLoss: tensor(1.4661, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 155 \tLoss: tensor(1.4874, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 156 \tLoss: tensor(1.4673, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 157 \tLoss: tensor(1.4704, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 158 \tLoss: tensor(1.4843, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 159 \tLoss: tensor(1.4834, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 160 \tLoss: tensor(1.4840, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 161 \tLoss: tensor(1.4670, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 162 \tLoss: tensor(1.4755, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 163 \tLoss: tensor(1.4869, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 164 \tLoss: tensor(1.4732, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 165 \tLoss: tensor(1.4683, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 166 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 167 \tLoss: tensor(1.4680, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 168 \tLoss: tensor(1.4776, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 169 \tLoss: tensor(1.4763, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 170 \tLoss: tensor(1.4737, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 171 \tLoss: tensor(1.4872, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 172 \tLoss: tensor(1.4977, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 173 \tLoss: tensor(1.4850, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 174 \tLoss: tensor(1.4735, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 175 \tLoss: tensor(1.4802, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 176 \tLoss: tensor(1.4853, grad_fn=<NllLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7 \tBatch: 177 \tLoss: tensor(1.4779, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 178 \tLoss: tensor(1.4948, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 179 \tLoss: tensor(1.4804, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 180 \tLoss: tensor(1.4745, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 181 \tLoss: tensor(1.4918, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 182 \tLoss: tensor(1.4837, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 183 \tLoss: tensor(1.4893, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 184 \tLoss: tensor(1.4881, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 185 \tLoss: tensor(1.4817, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 186 \tLoss: tensor(1.4719, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 187 \tLoss: tensor(1.4765, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 188 \tLoss: tensor(1.4757, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 189 \tLoss: tensor(1.4795, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 190 \tLoss: tensor(1.4835, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 191 \tLoss: tensor(1.4769, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 192 \tLoss: tensor(1.4806, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 193 \tLoss: tensor(1.4879, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 194 \tLoss: tensor(1.4715, grad_fn=<NllLossBackward>)\n",
      "Epoch: 7 \tBatch: 195 \tLoss: tensor(1.4736, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "my_nn = NN()\n",
    "\n",
    "# CREATE OUR OPTIMISER\n",
    "optimiser = torch.optim.Adam(              # what optimiser should we use?\n",
    "    my_nn.parameters(),          # what should it optimise?\n",
    "    lr=learning_rate                       # using what learning rate?\n",
    ")\n",
    "\n",
    "# CREATE OUR CRITERION\n",
    "criterion = torch.nn.CrossEntropyLoss()             # callable class that compares our predictions to our labels and returns our loss\n",
    "\n",
    "# SET UP TRAINING VISUALISATION\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()                            # we will use this to show our models performance on a graph\n",
    "    \n",
    "# TRAINING LOOP\n",
    "def train(model, epochs):\n",
    "    model.train()                                  # put the model into training mode (more on this later)\n",
    "    for epoch in range(epochs):\n",
    "        for idx, minibatch in enumerate(train_loader):\n",
    "            inputs, labels = minibatch\n",
    "            prediction = model(inputs)             # pass the data forward through the model\n",
    "            loss = criterion(prediction, labels)   # compute the loss\n",
    "            print('Epoch:', epoch, '\\tBatch:', idx, '\\tLoss:', loss)\n",
    "            optimiser.zero_grad()                  # reset the gradients attribute of each of the model's params to zero\n",
    "            loss.backward()                        # backward pass to compute and set all of the model param's gradients\n",
    "            optimiser.step()                       # update the model's parameters\n",
    "            writer.add_scalar('Loss/Train', loss, epoch*len(train_loader) + idx)    # write loss to a graph\n",
    "\n",
    "train(my_nn, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What does the loss actually mean practically?\n",
    "\n",
    "The absolute value of the loss doesn't really mean much, it's just a way of continuously evaluating the relative performance of the model whilst it trains. The real metric of performance that we care about is the proportion of ***unseen*** examples that our neural network can correctly classify. These unseen examples are what the test loader consists of.\n",
    "\n",
    "Let's write the code to calculate that now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.33000000000001\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "            \n",
    "def test(model):\n",
    "    num_correct = 0\n",
    "    num_examples = len(test_data)                       # test DATA not test LOADER\n",
    "    for inputs, labels in test_loader:                  # for all exampls, over all mini-batches in the test dataset\n",
    "        predictions = model(inputs)\n",
    "        predictions = torch.max(predictions, axis=1)    # reduce to find max indices along direction which column varies\n",
    "        predictions = predictions[1]                    # torch.max returns (values, indices)\n",
    "        num_correct += int(sum(predictions == labels))\n",
    "    percent_correct = num_correct / num_examples * 100\n",
    "    print('Accuracy:', percent_correct)\n",
    "    \n",
    "test(my_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "1. Compare the loss curves generated by using different batch sizes. What's the best? As you change the batch size, what variable do you need to change to give those curves the same domain over the x-axis (num writes to summary writer)\n",
    "2. It would be good to validate our model as we go along to ensure that we don't overfit. Let's write a training loop that tests the loss on the validation set after each epoch. Plot the validation error alongside What can you see on the graphs that indicates overfitting?\n",
    "3. What is the best accuracy you can achieve? Can you implement a grid search and a random search to try them automatically. Record all permutations that you try.\n",
    "4. What feature of the input data is our standard neural network not taking advantage of? Hint: '************* neural networks' take this into account.\n",
    "\n",
    "## Congratulations you boss, you've finished the notebook!\n",
    "\n",
    "Please provide your feedback [here](https://docs.google.com/forms/d/e/1FAIpQLSdZSxvkAE19vjDN4jpp0VvUBPGr_wdtayGAcRNfFGH7e7jQDQ/viewform?usp=sf_link). It means a lot to us.\n",
    "\n",
    "Next, you might want to check out:\n",
    "- [Convolutional Neural Networks](https://github.com/AI-Core/Convolutional-Neural-Networks/blob/master/Convolutional%20Neural%20Networks.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
